<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://smkang96.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://smkang96.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-11-12T13:28:49+00:00</updated><id>https://smkang96.github.io/feed.xml</id><title type="html">blank</title><subtitle>Sungmin&apos;s Blog
</subtitle><entry><title type="html">What do we know about developer debugging behavior?</title><link href="https://smkang96.github.io/blog/2023/what-we-know-about-debugging/" rel="alternate" type="text/html" title="What do we know about developer debugging behavior?" /><published>2023-10-06T13:17:00+00:00</published><updated>2023-10-06T13:17:00+00:00</updated><id>https://smkang96.github.io/blog/2023/what-we-know-about-debugging</id><content type="html" xml:base="https://smkang96.github.io/blog/2023/what-we-know-about-debugging/"><![CDATA[<blockquote>
  <p>TL;DR: While some papers argue that we don’t know how developers actually debug, there is actually quite a bit of observational research on developers debugging.</p>
</blockquote>]]></content><author><name></name></author><category term="debugging" /><category term="mini_survey" /><summary type="html"><![CDATA[In fact, some behavioral patterns recur when we look at the relevant literature.]]></summary></entry><entry><title type="html">[ICSE 2023] Exploring LLM-based General Bug Reproduction</title><link href="https://smkang96.github.io/blog/2023/libro-explained/" rel="alternate" type="text/html" title="[ICSE 2023] Exploring LLM-based General Bug Reproduction" /><published>2023-08-27T07:08:00+00:00</published><updated>2023-08-27T07:08:00+00:00</updated><id>https://smkang96.github.io/blog/2023/libro-explained</id><content type="html" xml:base="https://smkang96.github.io/blog/2023/libro-explained/"><![CDATA[<p>(Associated files, such as the bibtex or slides, can be found <a href="/al-folio/publications/#Kang2023aa">here</a>.)</p>

<p>TLDR: This paper introduces <code class="language-plaintext highlighter-rouge">LIBRO</code>, a technique that automatically reproduces bug reports with high precision.</p>

<h2 id="motivation">Motivation</h2>

<p>When users or developers find bugs, they can report these bugs on issue tracker software, such as on GitHub as issues or on Jira. Such reports are called “bug reports”. Bug reports are generally written in natural language, and may be submitted without a <strong>test</strong> that reproduces the issue. This is a difficult situation for developers, as they will then have to manually try to reproduce the issue in the bug report by writing their own test code. Given the large volume of bug reports being submitted every day, this burden can be overwhelming. Thus, bug reproduction is a situation where automatic test generation could help developers.</p>

<p>Despite this, due to the difficulty of natural language processing, automatic bug reproduction has been mainly limited to crash bugs, where the expected behavior is clear - the program should not crash. This actually leaves some simple cases (from the human perspective) difficult to automatically handle for machines. For example, consider the following <a href="https://issues.apache.org/jira/browse/MATH-370">bug report</a> from Commons Math:</p>

<blockquote>
  <p><strong>NaN in “equals” methods</strong></p>

  <p>In “MathUtils”, some “equals” methods will return true if both argument are NaN.
Unless I’m mistaken, this contradicts the IEEE standard.</p>

  <p>If nobody objects, I’m going to make the changes.</p>
</blockquote>

<p>In this case, it is clear what the expected behavior is to a human (<code class="language-plaintext highlighter-rouge">equals</code> methods should return false if both arguments are NaN). However, when trying to deal with this report using a rule-based algorithm, you immediately face two difficulties. First, the report does not explicitly spell out the expected behavior, so it is difficult to extract the expected behavior. Second, even if you get the oracle, all the code references in the report are disconnected, so it can be difficult to automatically write a test based on the available information. It is likely such difficulties that made automatic bug reproduction outside of crash bugs difficult.</p>

<h2 id="approach-and-results">Approach and Results</h2>

<p>It is no longer news, but one of the observations of our paper is that large language models (LLMs) do quite a good job of overcoming these challenges. After all, LLMs are good enough at natural language processing to the extent that they are capable of <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html">explaining humor</a>, and they are capable of generating code as well. Consequently, they are a natural solution to the test reproduction problem.</p>

<p>However, to build a tool that is actually useful to developers, simply generating many solutions from an LLM is not enough - we can’t hand a hundred candidate reproducing tests to a developer and say “job done”. Thankfully, for automatic bug reproduction, we find features that can help reduce this manual inspection cost. For example, a bug reproducing test should actually fail on the current, buggy version of the code, which allows us to automatically filter a portion of the unhelpful solutions. We bundled this heuristic, along with others, as an automated post-processing pipeline, specialized for handling bug reproduction.</p>

<p>With LLMs and the post-processing, which together we group in the tool <code class="language-plaintext highlighter-rouge">LIBRO</code>, we could both reproduce <strong>one-third</strong> of all of the bugs in the Defects4J benchmark, which is a substantially better performance than the baselines that we compared against. Furthermore, with the postprocessing pipeline we could achieve a precision of up to 0.85, reducing developer hassle with false positives.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup></p>

<h2 id="perspective-a-year-after-submitting-the-paper">Perspective A Year After Submitting The Paper</h2>

<p>Since then, there has been a substantial amount of work on test generation using large language models. Testing does seem to be one of the most natural applications of LLMs for software engineering - while LLMs have the tendency to be wrong, it is okay for tests to be “wrong”, as this may actually lead to the testing of interesting behavior, as we note in <a href="https://arxiv.org/abs/2306.05152">Robert et al.</a>.</p>

<p>If I can leave one takeaway for any readers who have come this far, it is that while LLM results are certainly valuable, the generation results of LLMs can be much more valuable when they are evaluated by a rule-based system, as once this is done, it is easier for humans to have trust in such results. This is a theme of much of my subsequent work on LLMs: I argued this point in my <a href="/al-folio/publications/#Kang2023lg">GI workshop position paper</a>, and explicitly made this a central theme of my <a href="/al-folio/publications/#kang2023explainable">Automatic Scientific Debugging</a>.</p>

<h3 id="footnotes">Footnotes</h3>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>In <a href="https://dl.acm.org/doi/10.1145/2931037.2931051">prior work</a> on fault localization, most developers reported that they would be satisfied with an FL tool with an accuracy of ca. 90%, so it is feasible that developers would be satisfied with the precision of <code class="language-plaintext highlighter-rouge">LIBRO</code>. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="academia" /><category term="LLM" /><category term="test_generation" /><summary type="html"><![CDATA[An explanation of my ICSE'23 work, and perspective one year after submitting the paper]]></summary></entry></feed>